{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Table Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.2  -0.07]\n",
      "The current number of episodes : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 01:11:48.483 Python[54245:2307203] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2024-11-10 01:11:48.483 Python[54245:2307203] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We made it episode 0\n",
      "The current number of episodes : 1\n",
      "We made it episode 1\n",
      "The current number of episodes : 2\n",
      "We made it episode 2\n",
      "The current number of episodes : 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     49\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(q_table[discrete_state])\n\u001b[0;32m---> 50\u001b[0m     new_state, reward, done, trun_, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     new_discrete_state \u001b[38;5;241m=\u001b[39m get_discrete_state(new_state)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m render:\n",
      "File \u001b[0;32m~/Desktop/College/Research-2024/venv/lib/python3.12/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Desktop/College/Research-2024/venv/lib/python3.12/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/College/Research-2024/venv/lib/python3.12/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/College/Research-2024/venv/lib/python3.12/site-packages/gymnasium/envs/classic_control/mountain_car.py:148\u001b[0m, in \u001b[0;36mMountainCarEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m (position, velocity)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/Desktop/College/Research-2024/venv/lib/python3.12/site-packages/gymnasium/envs/classic_control/mountain_car.py:266\u001b[0m, in \u001b[0;36mMountainCarEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\", render_mode='human')\n",
    "\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount = 0.95\n",
    "episodes = 25000\n",
    "show_every = 2000\n",
    "\n",
    "epsilon = 0.5\n",
    "start_epsilon_decay = 1\n",
    "end_epsilon_decay = episodes // 2\n",
    "\n",
    "epsilon_decay_value = (epsilon/(end_epsilon_decay - start_epsilon_decay))\n",
    "\n",
    "\n",
    "# print(env.observation_space.high)\n",
    "# print(env.observation_space.low)\n",
    "# print(env.action_space)\n",
    "\n",
    "\n",
    "\n",
    "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
    "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
    "\n",
    "\n",
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "\n",
    "\n",
    "print(env.observation_space.low)\n",
    "def get_discrete_state(state):\n",
    "    discrete_state = (state - env.observation_space.low) / discrete_os_win_size\n",
    "    return tuple(discrete_state.astype(int))\n",
    "\n",
    "for episode in range(episodes):\n",
    "    print(f\"The current number of episodes : {episode}\")\n",
    "    if episode % show_every == 0:\n",
    "        render = True\n",
    "    else:\n",
    "        render = False\n",
    "\n",
    "    state, info = env.reset()\n",
    "    discrete_state = get_discrete_state(state)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[discrete_state])\n",
    "        new_state, reward, done, trun_, info = env.step(action)\n",
    "\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "        if render:\n",
    "            env.render()\n",
    "        if not done:\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "            current_q = q_table[discrete_state + (action, )]\n",
    "            new_q = (1 - learning_rate) * current_q + learning_rate * (reward + discount * max_future_q)\n",
    "            q_table[discrete_state+(action, )] = new_q\n",
    "        elif new_state[0] >= env.unwrapped.goal_position:\n",
    "            print(f\"We made it episode {episode}\")\n",
    "            q_table[discrete_state + (action, )] = 0\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    if end_epsilon_decay >= epsilon >= start_epsilon_decay:\n",
    "        epsilon -= epsilon_decay_value\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.2  -0.07]\n",
      "The current number of episodes: 0\n",
      "Episode: 0, Epsilon: 0.1\n",
      "The current number of episodes: 1\n",
      "The current number of episodes: 2\n",
      "The current number of episodes: 3\n",
      "The current number of episodes: 4\n",
      "The current number of episodes: 5\n",
      "The current number of episodes: 6\n",
      "The current number of episodes: 7\n",
      "The current number of episodes: 8\n",
      "The current number of episodes: 9\n",
      "The current number of episodes: 10\n",
      "The current number of episodes: 11\n",
      "The current number of episodes: 12\n",
      "The current number of episodes: 13\n",
      "The current number of episodes: 14\n",
      "The current number of episodes: 15\n",
      "The current number of episodes: 16\n",
      "The current number of episodes: 17\n",
      "The current number of episodes: 18\n",
      "The current number of episodes: 19\n",
      "The current number of episodes: 20\n",
      "The current number of episodes: 21\n",
      "The current number of episodes: 22\n",
      "The current number of episodes: 23\n",
      "The current number of episodes: 24\n",
      "The current number of episodes: 25\n",
      "The current number of episodes: 26\n",
      "The current number of episodes: 27\n",
      "The current number of episodes: 28\n",
      "The current number of episodes: 29\n",
      "The current number of episodes: 30\n",
      "The current number of episodes: 31\n",
      "The current number of episodes: 32\n",
      "The current number of episodes: 33\n",
      "The current number of episodes: 34\n",
      "The current number of episodes: 35\n",
      "The current number of episodes: 36\n",
      "The current number of episodes: 37\n",
      "The current number of episodes: 38\n",
      "The current number of episodes: 39\n",
      "The current number of episodes: 40\n",
      "The current number of episodes: 41\n",
      "The current number of episodes: 42\n",
      "The current number of episodes: 43\n",
      "The current number of episodes: 44\n",
      "The current number of episodes: 45\n",
      "The current number of episodes: 46\n",
      "The current number of episodes: 47\n",
      "The current number of episodes: 48\n",
      "The current number of episodes: 49\n",
      "The current number of episodes: 50\n",
      "The current number of episodes: 51\n",
      "The current number of episodes: 52\n",
      "The current number of episodes: 53\n",
      "The current number of episodes: 54\n",
      "The current number of episodes: 55\n",
      "The current number of episodes: 56\n",
      "The current number of episodes: 57\n",
      "The current number of episodes: 58\n",
      "The current number of episodes: 59\n",
      "The current number of episodes: 60\n",
      "The current number of episodes: 61\n",
      "The current number of episodes: 62\n",
      "The current number of episodes: 63\n",
      "The current number of episodes: 64\n",
      "The current number of episodes: 65\n",
      "The current number of episodes: 66\n",
      "The current number of episodes: 67\n",
      "The current number of episodes: 68\n",
      "The current number of episodes: 69\n",
      "The current number of episodes: 70\n",
      "The current number of episodes: 71\n",
      "The current number of episodes: 72\n",
      "The current number of episodes: 73\n",
      "The current number of episodes: 74\n",
      "The current number of episodes: 75\n",
      "The current number of episodes: 76\n",
      "The current number of episodes: 77\n",
      "The current number of episodes: 78\n",
      "The current number of episodes: 79\n",
      "The current number of episodes: 80\n",
      "The current number of episodes: 81\n",
      "The current number of episodes: 82\n",
      "The current number of episodes: 83\n",
      "The current number of episodes: 84\n",
      "The current number of episodes: 85\n",
      "The current number of episodes: 86\n",
      "The current number of episodes: 87\n",
      "The current number of episodes: 88\n",
      "The current number of episodes: 89\n",
      "The current number of episodes: 90\n",
      "The current number of episodes: 91\n",
      "The current number of episodes: 92\n",
      "The current number of episodes: 93\n",
      "The current number of episodes: 94\n",
      "The current number of episodes: 95\n",
      "The current number of episodes: 96\n",
      "The current number of episodes: 97\n",
      "The current number of episodes: 98\n",
      "The current number of episodes: 99\n",
      "The current number of episodes: 100\n",
      "The current number of episodes: 101\n",
      "The current number of episodes: 102\n",
      "The current number of episodes: 103\n",
      "The current number of episodes: 104\n",
      "The current number of episodes: 105\n",
      "The current number of episodes: 106\n",
      "The current number of episodes: 107\n",
      "The current number of episodes: 108\n",
      "The current number of episodes: 109\n",
      "The current number of episodes: 110\n",
      "The current number of episodes: 111\n",
      "The current number of episodes: 112\n",
      "The current number of episodes: 113\n",
      "The current number of episodes: 114\n",
      "The current number of episodes: 115\n",
      "The current number of episodes: 116\n",
      "The current number of episodes: 117\n",
      "The current number of episodes: 118\n",
      "The current number of episodes: 119\n",
      "The current number of episodes: 120\n",
      "The current number of episodes: 121\n",
      "The current number of episodes: 122\n",
      "The current number of episodes: 123\n",
      "The current number of episodes: 124\n",
      "The current number of episodes: 125\n",
      "The current number of episodes: 126\n",
      "The current number of episodes: 127\n",
      "The current number of episodes: 128\n",
      "The current number of episodes: 129\n",
      "The current number of episodes: 130\n",
      "The current number of episodes: 131\n",
      "The current number of episodes: 132\n",
      "The current number of episodes: 133\n",
      "The current number of episodes: 134\n",
      "The current number of episodes: 135\n",
      "The current number of episodes: 136\n",
      "The current number of episodes: 137\n",
      "The current number of episodes: 138\n",
      "The current number of episodes: 139\n",
      "The current number of episodes: 140\n",
      "The current number of episodes: 141\n",
      "The current number of episodes: 142\n",
      "The current number of episodes: 143\n",
      "The current number of episodes: 144\n",
      "The current number of episodes: 145\n",
      "The current number of episodes: 146\n",
      "The current number of episodes: 147\n",
      "The current number of episodes: 148\n",
      "The current number of episodes: 149\n",
      "The current number of episodes: 150\n",
      "The current number of episodes: 151\n",
      "The current number of episodes: 152\n",
      "The current number of episodes: 153\n",
      "The current number of episodes: 154\n",
      "The current number of episodes: 155\n",
      "The current number of episodes: 156\n",
      "The current number of episodes: 157\n",
      "The current number of episodes: 158\n",
      "The current number of episodes: 159\n",
      "The current number of episodes: 160\n",
      "The current number of episodes: 161\n",
      "The current number of episodes: 162\n",
      "The current number of episodes: 163\n",
      "The current number of episodes: 164\n",
      "The current number of episodes: 165\n",
      "The current number of episodes: 166\n",
      "The current number of episodes: 167\n",
      "The current number of episodes: 168\n",
      "The current number of episodes: 169\n",
      "The current number of episodes: 170\n",
      "The current number of episodes: 171\n",
      "The current number of episodes: 172\n",
      "The current number of episodes: 173\n",
      "The current number of episodes: 174\n",
      "The current number of episodes: 175\n",
      "The current number of episodes: 176\n",
      "The current number of episodes: 177\n",
      "The current number of episodes: 178\n",
      "The current number of episodes: 179\n",
      "The current number of episodes: 180\n",
      "The current number of episodes: 181\n",
      "The current number of episodes: 182\n",
      "The current number of episodes: 183\n",
      "The current number of episodes: 184\n",
      "The current number of episodes: 185\n",
      "The current number of episodes: 186\n",
      "The current number of episodes: 187\n",
      "The current number of episodes: 188\n",
      "The current number of episodes: 189\n",
      "The current number of episodes: 190\n",
      "The current number of episodes: 191\n",
      "The current number of episodes: 192\n",
      "The current number of episodes: 193\n",
      "The current number of episodes: 194\n",
      "The current number of episodes: 195\n",
      "The current number of episodes: 196\n",
      "The current number of episodes: 197\n",
      "The current number of episodes: 198\n",
      "The current number of episodes: 199\n",
      "The current number of episodes: 200\n",
      "The current number of episodes: 201\n",
      "The current number of episodes: 202\n",
      "The current number of episodes: 203\n",
      "The current number of episodes: 204\n",
      "The current number of episodes: 205\n",
      "The current number of episodes: 206\n",
      "The current number of episodes: 207\n",
      "The current number of episodes: 208\n",
      "The current number of episodes: 209\n",
      "The current number of episodes: 210\n",
      "The current number of episodes: 211\n",
      "The current number of episodes: 212\n",
      "The current number of episodes: 213\n",
      "The current number of episodes: 214\n",
      "The current number of episodes: 215\n",
      "The current number of episodes: 216\n",
      "The current number of episodes: 217\n",
      "The current number of episodes: 218\n",
      "The current number of episodes: 219\n",
      "The current number of episodes: 220\n",
      "The current number of episodes: 221\n",
      "The current number of episodes: 222\n",
      "The current number of episodes: 223\n",
      "The current number of episodes: 224\n",
      "The current number of episodes: 225\n",
      "The current number of episodes: 226\n",
      "The current number of episodes: 227\n",
      "The current number of episodes: 228\n",
      "The current number of episodes: 229\n",
      "The current number of episodes: 230\n",
      "The current number of episodes: 231\n",
      "The current number of episodes: 232\n",
      "The current number of episodes: 233\n",
      "The current number of episodes: 234\n",
      "The current number of episodes: 235\n",
      "The current number of episodes: 236\n",
      "The current number of episodes: 237\n",
      "The current number of episodes: 238\n",
      "The current number of episodes: 239\n",
      "The current number of episodes: 240\n",
      "The current number of episodes: 241\n",
      "The current number of episodes: 242\n",
      "The current number of episodes: 243\n",
      "The current number of episodes: 244\n",
      "The current number of episodes: 245\n",
      "The current number of episodes: 246\n",
      "The current number of episodes: 247\n",
      "The current number of episodes: 248\n",
      "The current number of episodes: 249\n",
      "The current number of episodes: 250\n",
      "The current number of episodes: 251\n",
      "The current number of episodes: 252\n",
      "The current number of episodes: 253\n",
      "The current number of episodes: 254\n",
      "The current number of episodes: 255\n",
      "The current number of episodes: 256\n",
      "The current number of episodes: 257\n",
      "The current number of episodes: 258\n",
      "The current number of episodes: 259\n",
      "The current number of episodes: 260\n",
      "The current number of episodes: 261\n",
      "The current number of episodes: 262\n",
      "The current number of episodes: 263\n",
      "The current number of episodes: 264\n",
      "The current number of episodes: 265\n",
      "The current number of episodes: 266\n",
      "The current number of episodes: 267\n",
      "The current number of episodes: 268\n",
      "The current number of episodes: 269\n",
      "The current number of episodes: 270\n",
      "The current number of episodes: 271\n",
      "The current number of episodes: 272\n",
      "The current number of episodes: 273\n",
      "The current number of episodes: 274\n",
      "The current number of episodes: 275\n",
      "The current number of episodes: 276\n",
      "The current number of episodes: 277\n",
      "The current number of episodes: 278\n",
      "The current number of episodes: 279\n",
      "The current number of episodes: 280\n",
      "The current number of episodes: 281\n",
      "The current number of episodes: 282\n",
      "The current number of episodes: 283\n",
      "The current number of episodes: 284\n",
      "The current number of episodes: 285\n",
      "The current number of episodes: 286\n",
      "The current number of episodes: 287\n",
      "The current number of episodes: 288\n",
      "The current number of episodes: 289\n",
      "The current number of episodes: 290\n",
      "The current number of episodes: 291\n",
      "The current number of episodes: 292\n",
      "The current number of episodes: 293\n",
      "The current number of episodes: 294\n",
      "The current number of episodes: 295\n",
      "The current number of episodes: 296\n",
      "The current number of episodes: 297\n",
      "The current number of episodes: 298\n",
      "The current number of episodes: 299\n",
      "The current number of episodes: 300\n",
      "The current number of episodes: 301\n",
      "The current number of episodes: 302\n",
      "The current number of episodes: 303\n",
      "The current number of episodes: 304\n",
      "The current number of episodes: 305\n",
      "The current number of episodes: 306\n",
      "The current number of episodes: 307\n",
      "The current number of episodes: 308\n",
      "The current number of episodes: 309\n",
      "The current number of episodes: 310\n",
      "The current number of episodes: 311\n",
      "The current number of episodes: 312\n",
      "The current number of episodes: 313\n",
      "The current number of episodes: 314\n",
      "The current number of episodes: 315\n",
      "The current number of episodes: 316\n",
      "The current number of episodes: 317\n",
      "The current number of episodes: 318\n",
      "The current number of episodes: 319\n",
      "The current number of episodes: 320\n",
      "The current number of episodes: 321\n",
      "The current number of episodes: 322\n",
      "The current number of episodes: 323\n",
      "The current number of episodes: 324\n",
      "The current number of episodes: 325\n",
      "The current number of episodes: 326\n",
      "The current number of episodes: 327\n",
      "The current number of episodes: 328\n",
      "The current number of episodes: 329\n",
      "The current number of episodes: 330\n",
      "We made it on episode 330\n",
      "The current number of episodes: 331\n",
      "The current number of episodes: 332\n",
      "The current number of episodes: 333\n",
      "The current number of episodes: 334\n",
      "The current number of episodes: 335\n",
      "The current number of episodes: 336\n",
      "The current number of episodes: 337\n",
      "The current number of episodes: 338\n",
      "The current number of episodes: 339\n",
      "The current number of episodes: 340\n",
      "The current number of episodes: 341\n",
      "The current number of episodes: 342\n",
      "The current number of episodes: 343\n",
      "The current number of episodes: 344\n",
      "The current number of episodes: 345\n",
      "The current number of episodes: 346\n",
      "The current number of episodes: 347\n",
      "The current number of episodes: 348\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\", render_mode='human')\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount = 0.95\n",
    "episodes = 25000\n",
    "show_every = 2000\n",
    "\n",
    "epsilon = 0.1\n",
    "start_epsilon_decay = 1\n",
    "end_epsilon_decay = episodes // 2\n",
    "\n",
    "epsilon_decay_value = epsilon / (end_epsilon_decay - start_epsilon_decay)\n",
    "\n",
    "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
    "discrete_os_win_size = (env.observation_space.high - env.observation_space.low) / DISCRETE_OS_SIZE\n",
    "\n",
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "\n",
    "print(env.observation_space.low)\n",
    "\n",
    "def get_discrete_state(state):\n",
    "    discrete_state = (state - env.observation_space.low) / discrete_os_win_size\n",
    "    return tuple(discrete_state.astype(int))\n",
    "\n",
    "for episode in range(episodes):\n",
    "    print(f\"The current number of episodes: {episode}\")\n",
    "    if episode % show_every == 0:\n",
    "        render = True\n",
    "        print(f\"Episode: {episode}, Epsilon: {epsilon}\")\n",
    "    else:\n",
    "        render = False\n",
    "\n",
    "    state, info = env.reset()\n",
    "    discrete_state = get_discrete_state(state)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Implement epsilon-greedy action selection\n",
    "        if np.random.random() > epsilon:\n",
    "            # Exploit: select the best-known action\n",
    "            action = np.argmax(q_table[discrete_state])\n",
    "        else:\n",
    "            # Explore: select a random action\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "\n",
    "        # Correctly unpack the step function's return values\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        # Q-learning algorithm\n",
    "        if not done:\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "            current_q = q_table[discrete_state + (action, )]\n",
    "            # Standard Q-learning update rule\n",
    "            new_q = (1 - learning_rate) * current_q + learning_rate * (reward + discount * max_future_q)\n",
    "            # Update Q-value for the current state-action pair\n",
    "            q_table[discrete_state + (action, )] = new_q\n",
    "        elif new_state[0] >= env.unwrapped.goal_position:\n",
    "            print(f\"We made it on episode {episode}\")\n",
    "            # Set Q-value to zero if goal is reached\n",
    "            q_table[discrete_state + (action, )] = 0\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    # Epsilon decay\n",
    "    if end_epsilon_decay >= episode >= start_epsilon_decay:\n",
    "        epsilon -= epsilon_decay_value\n",
    "        epsilon = max(epsilon, 0)\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
